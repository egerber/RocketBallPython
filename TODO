
  TODO:
  - generate good trainigdata for forward model 2
  - train rocketBall with non zero initial state
  - change move() procedures to take arrays
  - select available configurations from list and create corresponding path automatically
  - change log strategy of forward Model to save less datapoints

QUESTION:
- how to initialize LSTM state when training on LabyrinthGrid?
- should the parameters for adam be updated?
- initial state of LSTM should be encoded position (but how is it encoded?)
- Why is the error displayed in tensorboard always by several magnitudes higher than the error displayed in the commandline after each training cycle
-> probably the error in the command window is wrong because the results are usually not as good as they should.
@Inference: after each cycle set inputs to one_hot?

INSIGHTS
- Batchsize=1 works best


TODO today
- train (128) network with one configurations (try seed 1 and 2)
- train (16,128) network on larger training data (10.000 per configuration)
-> use extremely small and extremely large learning rate
- train smaller network (5x5)

